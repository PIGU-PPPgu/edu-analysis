#!/bin/bash

# ğŸš€ éƒ¨ç½²ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿè„šæœ¬
# è‡ªåŠ¨åŒ–éƒ¨ç½²Prometheusã€Grafanaã€AlertManagerã€ELKæ ˆç­‰ç›‘æ§ç»„ä»¶

set -euo pipefail

# è„šæœ¬é…ç½®
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
PROJECT_ROOT="$(dirname "$SCRIPT_DIR")"
K8S_DIR="$PROJECT_ROOT/k8s"
MONITORING_DIR="$K8S_DIR/monitoring"
LOGGING_DIR="$K8S_DIR/logging"

# é¢œè‰²è¾“å‡º
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# æ—¥å¿—å‡½æ•°
log() {
    echo -e "${GREEN}[$(date +'%Y-%m-%d %H:%M:%S')] $1${NC}"
}

warn() {
    echo -e "${YELLOW}[$(date +'%Y-%m-%d %H:%M:%S')] WARNING: $1${NC}"
}

error() {
    echo -e "${RED}[$(date +'%Y-%m-%d %H:%M:%S')] ERROR: $1${NC}"
    exit 1
}

info() {
    echo -e "${BLUE}[$(date +'%Y-%m-%d %H:%M:%S')] INFO: $1${NC}"
}

# æ£€æŸ¥ä¾èµ–
check_dependencies() {
    log "æ£€æŸ¥ç³»ç»Ÿä¾èµ–..."
    
    # æ£€æŸ¥kubectl
    if ! command -v kubectl &> /dev/null; then
        error "kubectl æœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£… kubectl"
    fi
    
    # æ£€æŸ¥helm (å¯é€‰)
    if command -v helm &> /dev/null; then
        info "æ£€æµ‹åˆ° Helmï¼Œå°†ä½¿ç”¨ Helm è¿›è¡Œéƒ¨åˆ†éƒ¨ç½²"
        HELM_AVAILABLE=true
    else
        warn "æœªæ£€æµ‹åˆ° Helmï¼Œå°†ä½¿ç”¨çº¯ kubectl éƒ¨ç½²"
        HELM_AVAILABLE=false
    fi
    
    # æ£€æŸ¥é›†ç¾¤è¿æ¥
    if ! kubectl cluster-info &> /dev/null; then
        error "æ— æ³•è¿æ¥åˆ° Kubernetes é›†ç¾¤ï¼Œè¯·æ£€æŸ¥ kubeconfig"
    fi
    
    log "âœ… ä¾èµ–æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºå‘½åç©ºé—´
create_namespaces() {
    log "åˆ›å»ºå‘½åç©ºé—´..."
    
    # åˆ›å»ºç›‘æ§å‘½åç©ºé—´
    kubectl create namespace monitoring --dry-run=client -o yaml | kubectl apply -f -
    kubectl label namespace monitoring tier=infrastructure --overwrite
    
    # åˆ›å»ºæ—¥å¿—å‘½åç©ºé—´
    kubectl create namespace logging --dry-run=client -o yaml | kubectl apply -f -
    kubectl label namespace logging tier=infrastructure --overwrite
    
    log "âœ… å‘½åç©ºé—´åˆ›å»ºå®Œæˆ"
}

# éƒ¨ç½²å­˜å‚¨ç±»
deploy_storage_classes() {
    log "éƒ¨ç½²å­˜å‚¨ç±»..."
    
    # æ£€æŸ¥æ˜¯å¦å·²å­˜åœ¨å­˜å‚¨ç±»
    if kubectl get storageclass fast-ssd &> /dev/null; then
        info "å­˜å‚¨ç±» fast-ssd å·²å­˜åœ¨ï¼Œè·³è¿‡åˆ›å»º"
    else
        # åˆ›å»ºé«˜æ€§èƒ½SSDå­˜å‚¨ç±»
        cat <<EOF | kubectl apply -f -
apiVersion: storage.k8s.io/v1
kind: StorageClass
metadata:
  name: fast-ssd
  labels:
    app: monitoring
provisioner: kubernetes.io/aws-ebs
parameters:
  type: gp3
  iops: "3000"
  throughput: "125"
  fsType: ext4
allowVolumeExpansion: true
reclaimPolicy: Retain
volumeBindingMode: WaitForFirstConsumer
EOF
        log "âœ… å­˜å‚¨ç±»åˆ›å»ºå®Œæˆ"
    fi
}

# éƒ¨ç½²Prometheusç›‘æ§ç³»ç»Ÿ
deploy_prometheus() {
    log "éƒ¨ç½²Prometheusç›‘æ§ç³»ç»Ÿ..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$MONITORING_DIR/prometheus.yaml" ]]; then
        error "Prometheusé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $MONITORING_DIR/prometheus.yaml"
    fi
    
    # éƒ¨ç½²Prometheus
    kubectl apply -f "$MONITORING_DIR/prometheus.yaml"
    
    # ç­‰å¾…Prometheuså¯åŠ¨
    info "ç­‰å¾…Prometheus Podå¯åŠ¨..."
    kubectl wait --for=condition=Ready pod -l app=prometheus -n monitoring --timeout=300s
    
    # æ£€æŸ¥PrometheusçŠ¶æ€
    if kubectl get pods -n monitoring -l app=prometheus | grep -q "Running"; then
        log "âœ… Prometheuséƒ¨ç½²æˆåŠŸ"
        
        # è·å–Prometheusè®¿é—®ä¿¡æ¯
        info "Prometheusè®¿é—®ä¿¡æ¯:"
        info "- å†…éƒ¨è®¿é—®: http://prometheus.monitoring.svc.cluster.local:9090"
        info "- ç«¯å£è½¬å‘: kubectl port-forward -n monitoring svc/prometheus 9090:9090"
    else
        error "Prometheuséƒ¨ç½²å¤±è´¥"
    fi
}

# éƒ¨ç½²Grafanaå¯è§†åŒ–ç³»ç»Ÿ
deploy_grafana() {
    log "éƒ¨ç½²Grafanaå¯è§†åŒ–ç³»ç»Ÿ..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$MONITORING_DIR/grafana.yaml" ]]; then
        error "Grafanaé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $MONITORING_DIR/grafana.yaml"
    fi
    
    # éƒ¨ç½²Grafana
    kubectl apply -f "$MONITORING_DIR/grafana.yaml"
    
    # ç­‰å¾…Grafanaå¯åŠ¨
    info "ç­‰å¾…Grafana Podå¯åŠ¨..."
    kubectl wait --for=condition=Ready pod -l app=grafana -n monitoring --timeout=300s
    
    # æ£€æŸ¥GrafanaçŠ¶æ€
    if kubectl get pods -n monitoring -l app=grafana | grep -q "Running"; then
        log "âœ… Grafanaéƒ¨ç½²æˆåŠŸ"
        
        # è·å–Grafanaè®¿é—®ä¿¡æ¯
        info "Grafanaè®¿é—®ä¿¡æ¯:"
        info "- å†…éƒ¨è®¿é—®: http://grafana.monitoring.svc.cluster.local:3000"
        info "- ç«¯å£è½¬å‘: kubectl port-forward -n monitoring svc/grafana 3000:3000"
        info "- é»˜è®¤è´¦å·: admin / admin123!@#"
    else
        error "Grafanaéƒ¨ç½²å¤±è´¥"
    fi
}

# éƒ¨ç½²AlertManagerå‘Šè­¦ç³»ç»Ÿ
deploy_alertmanager() {
    log "éƒ¨ç½²AlertManagerå‘Šè­¦ç³»ç»Ÿ..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$MONITORING_DIR/alertmanager.yaml" ]]; then
        error "AlertManageré…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $MONITORING_DIR/alertmanager.yaml"
    fi
    
    # éƒ¨ç½²AlertManager
    kubectl apply -f "$MONITORING_DIR/alertmanager.yaml"
    
    # ç­‰å¾…AlertManagerå¯åŠ¨
    info "ç­‰å¾…AlertManager Podå¯åŠ¨..."
    kubectl wait --for=condition=Ready pod -l app=alertmanager -n monitoring --timeout=300s
    
    # æ£€æŸ¥AlertManagerçŠ¶æ€
    if kubectl get pods -n monitoring -l app=alertmanager | grep -q "Running"; then
        log "âœ… AlertManageréƒ¨ç½²æˆåŠŸ"
        
        # è·å–AlertManagerè®¿é—®ä¿¡æ¯
        info "AlertManagerè®¿é—®ä¿¡æ¯:"
        info "- å†…éƒ¨è®¿é—®: http://alertmanager.monitoring.svc.cluster.local:9093"
        info "- ç«¯å£è½¬å‘: kubectl port-forward -n monitoring svc/alertmanager 9093:9093"
    else
        error "AlertManageréƒ¨ç½²å¤±è´¥"
    fi
}

# éƒ¨ç½²Node Exporter
deploy_node_exporter() {
    log "éƒ¨ç½²Node Exporter..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$MONITORING_DIR/node-exporter.yaml" ]]; then
        error "Node Exporteré…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $MONITORING_DIR/node-exporter.yaml"
    fi
    
    # éƒ¨ç½²Node Exporter
    kubectl apply -f "$MONITORING_DIR/node-exporter.yaml"
    
    # ç­‰å¾…Node Exporterå¯åŠ¨
    info "ç­‰å¾…Node Exporter DaemonSetå¯åŠ¨..."
    kubectl rollout status daemonset/node-exporter -n monitoring --timeout=300s
    
    # æ£€æŸ¥Node ExporterçŠ¶æ€
    NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
    RUNNING_COUNT=$(kubectl get pods -n monitoring -l app=node-exporter --no-headers | grep Running | wc -l)
    
    if [[ "$RUNNING_COUNT" -eq "$NODE_COUNT" ]]; then
        log "âœ… Node Exporteréƒ¨ç½²æˆåŠŸ ($RUNNING_COUNT/$NODE_COUNT èŠ‚ç‚¹)"
    else
        warn "Node Exporteréƒ¨ç½²éƒ¨åˆ†æˆåŠŸ ($RUNNING_COUNT/$NODE_COUNT èŠ‚ç‚¹)"
    fi
}

# éƒ¨ç½²Elasticsearch
deploy_elasticsearch() {
    log "éƒ¨ç½²Elasticsearchæ—¥å¿—å­˜å‚¨..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$LOGGING_DIR/elasticsearch.yaml" ]]; then
        error "Elasticsearché…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $LOGGING_DIR/elasticsearch.yaml"
    fi
    
    # éƒ¨ç½²Elasticsearch
    kubectl apply -f "$LOGGING_DIR/elasticsearch.yaml"
    
    # ç­‰å¾…Elasticsearchå¯åŠ¨ï¼ˆå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼‰
    info "ç­‰å¾…Elasticsearché›†ç¾¤å¯åŠ¨ï¼ˆå¯èƒ½éœ€è¦5-10åˆ†é’Ÿï¼‰..."
    kubectl wait --for=condition=Ready pod -l app=elasticsearch -n logging --timeout=600s
    
    # æ£€æŸ¥ElasticsearchçŠ¶æ€
    if kubectl get pods -n logging -l app=elasticsearch | grep -q "Running"; then
        log "âœ… Elasticsearchéƒ¨ç½²æˆåŠŸ"
        
        # ç­‰å¾…é›†ç¾¤å°±ç»ª
        info "æ£€æŸ¥Elasticsearché›†ç¾¤å¥åº·çŠ¶æ€..."
        for i in {1..30}; do
            if kubectl exec -n logging statefulset/elasticsearch-master -- curl -s http://localhost:9200/_cluster/health | grep -q "yellow\|green"; then
                log "âœ… Elasticsearché›†ç¾¤å¥åº·çŠ¶æ€æ­£å¸¸"
                break
            fi
            info "ç­‰å¾…é›†ç¾¤å¥åº·æ£€æŸ¥... ($i/30)"
            sleep 10
        done
    else
        error "Elasticsearchéƒ¨ç½²å¤±è´¥"
    fi
}

# éƒ¨ç½²Logstash
deploy_logstash() {
    log "éƒ¨ç½²Logstashæ—¥å¿—å¤„ç†..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$LOGGING_DIR/logstash.yaml" ]]; then
        error "Logstashé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $LOGGING_DIR/logstash.yaml"
    fi
    
    # éƒ¨ç½²Logstash
    kubectl apply -f "$LOGGING_DIR/logstash.yaml"
    
    # ç­‰å¾…Logstashå¯åŠ¨
    info "ç­‰å¾…Logstash Podå¯åŠ¨..."
    kubectl wait --for=condition=Ready pod -l app=logstash -n logging --timeout=300s
    
    # æ£€æŸ¥LogstashçŠ¶æ€
    if kubectl get pods -n logging -l app=logstash | grep -q "Running"; then
        log "âœ… Logstashéƒ¨ç½²æˆåŠŸ"
    else
        error "Logstashéƒ¨ç½²å¤±è´¥"
    fi
}

# éƒ¨ç½²Kibana
deploy_kibana() {
    log "éƒ¨ç½²Kibanaæ—¥å¿—å¯è§†åŒ–..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$LOGGING_DIR/kibana.yaml" ]]; then
        error "Kibanaé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $LOGGING_DIR/kibana.yaml"
    fi
    
    # éƒ¨ç½²Kibana
    kubectl apply -f "$LOGGING_DIR/kibana.yaml"
    
    # ç­‰å¾…Kibanaå¯åŠ¨
    info "ç­‰å¾…Kibana Podå¯åŠ¨..."
    kubectl wait --for=condition=Ready pod -l app=kibana -n logging --timeout=300s
    
    # æ£€æŸ¥KibanaçŠ¶æ€
    if kubectl get pods -n logging -l app=kibana | grep -q "Running"; then
        log "âœ… Kibanaéƒ¨ç½²æˆåŠŸ"
        
        # è·å–Kibanaè®¿é—®ä¿¡æ¯
        info "Kibanaè®¿é—®ä¿¡æ¯:"
        info "- å†…éƒ¨è®¿é—®: http://kibana.logging.svc.cluster.local:5601"
        info "- ç«¯å£è½¬å‘: kubectl port-forward -n logging svc/kibana 5601:5601"
    else
        error "Kibanaéƒ¨ç½²å¤±è´¥"
    fi
}

# éƒ¨ç½²Filebeat
deploy_filebeat() {
    log "éƒ¨ç½²Filebeatæ—¥å¿—æ”¶é›†..."
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if [[ ! -f "$LOGGING_DIR/filebeat.yaml" ]]; then
        error "Filebeaté…ç½®æ–‡ä»¶ä¸å­˜åœ¨: $LOGGING_DIR/filebeat.yaml"
    fi
    
    # éƒ¨ç½²Filebeat
    kubectl apply -f "$LOGGING_DIR/filebeat.yaml"
    
    # ç­‰å¾…Filebeatå¯åŠ¨
    info "ç­‰å¾…Filebeat DaemonSetå¯åŠ¨..."
    kubectl rollout status daemonset/filebeat -n logging --timeout=300s
    
    # æ£€æŸ¥FilebeatçŠ¶æ€
    NODE_COUNT=$(kubectl get nodes --no-headers | wc -l)
    RUNNING_COUNT=$(kubectl get pods -n logging -l app=filebeat --no-headers | grep Running | wc -l)
    
    if [[ "$RUNNING_COUNT" -eq "$NODE_COUNT" ]]; then
        log "âœ… Filebeatéƒ¨ç½²æˆåŠŸ ($RUNNING_COUNT/$NODE_COUNT èŠ‚ç‚¹)"
    else
        warn "Filebeatéƒ¨ç½²éƒ¨åˆ†æˆåŠŸ ($RUNNING_COUNT/$NODE_COUNT èŠ‚ç‚¹)"
    fi
}

# é…ç½®ç›‘æ§å¯¹è±¡
configure_monitoring() {
    log "é…ç½®åº”ç”¨ç›‘æ§..."
    
    # ä¸ºåº”ç”¨Podæ·»åŠ Prometheusæ³¨è§£
    kubectl patch deployment figma-frame-faithful -n production --type='merge' -p='{
        "spec": {
            "template": {
                "metadata": {
                    "annotations": {
                        "prometheus.io/scrape": "true",
                        "prometheus.io/port": "80",
                        "prometheus.io/path": "/metrics"
                    }
                }
            }
        }
    }' || warn "åº”ç”¨deploymentä¸å­˜åœ¨ï¼Œè·³è¿‡ç›‘æ§é…ç½®"
    
    # ä¸ºåº”ç”¨Podæ·»åŠ æ—¥å¿—æ ‡ç­¾
    kubectl patch deployment figma-frame-faithful -n production --type='merge' -p='{
        "spec": {
            "template": {
                "metadata": {
                    "labels": {
                        "logging": "enabled"
                    }
                }
            }
        }
    }' || warn "åº”ç”¨deploymentä¸å­˜åœ¨ï¼Œè·³è¿‡æ—¥å¿—é…ç½®"
    
    log "âœ… ç›‘æ§é…ç½®å®Œæˆ"
}

# åˆ›å»ºIngress (å¯é€‰)
create_ingress() {
    if [[ "${ENABLE_INGRESS:-false}" == "true" ]]; then
        log "åˆ›å»ºIngressè®¿é—®å…¥å£..."
        
        # Grafana Ingress
        cat <<EOF | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: grafana
  namespace: monitoring
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/ssl-redirect: "true"
    cert-manager.io/cluster-issuer: letsencrypt-production
spec:
  tls:
  - hosts:
    - grafana.figma-frame-faithful.com
    secretName: grafana-tls-secret
  rules:
  - host: grafana.figma-frame-faithful.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend:
          service:
            name: grafana
            port:
              number: 3000
EOF
        
        log "âœ… Ingressåˆ›å»ºå®Œæˆ"
    fi
}

# éªŒè¯éƒ¨ç½²
verify_deployment() {
    log "éªŒè¯éƒ¨ç½²çŠ¶æ€..."
    
    echo
    info "=== ç›‘æ§ç³»ç»ŸçŠ¶æ€ ==="
    kubectl get pods -n monitoring -o wide
    
    echo
    info "=== æ—¥å¿—ç³»ç»ŸçŠ¶æ€ ==="
    kubectl get pods -n logging -o wide
    
    echo
    info "=== æœåŠ¡çŠ¶æ€ ==="
    kubectl get svc -n monitoring
    kubectl get svc -n logging
    
    echo
    info "=== å­˜å‚¨çŠ¶æ€ ==="
    kubectl get pvc -n monitoring
    kubectl get pvc -n logging
    
    # æ£€æŸ¥å…³é”®ç»„ä»¶å¥åº·çŠ¶æ€
    echo
    info "=== ç»„ä»¶å¥åº·æ£€æŸ¥ ==="
    
    # Prometheuså¥åº·æ£€æŸ¥
    if kubectl exec -n monitoring deployment/prometheus -- wget -q -O- http://localhost:9090/-/healthy 2>/dev/null | grep -q "Prometheus is Healthy"; then
        log "âœ… Prometheus å¥åº·çŠ¶æ€æ­£å¸¸"
    else
        warn "âŒ Prometheus å¥åº·æ£€æŸ¥å¤±è´¥"
    fi
    
    # Grafanaå¥åº·æ£€æŸ¥
    if kubectl exec -n monitoring deployment/grafana -- curl -s http://localhost:3000/api/health 2>/dev/null | grep -q "ok"; then
        log "âœ… Grafana å¥åº·çŠ¶æ€æ­£å¸¸"
    else
        warn "âŒ Grafana å¥åº·æ£€æŸ¥å¤±è´¥"
    fi
    
    # Elasticsearchå¥åº·æ£€æŸ¥
    if kubectl exec -n logging statefulset/elasticsearch-master -- curl -s http://localhost:9200/_cluster/health 2>/dev/null | grep -q "yellow\|green"; then
        log "âœ… Elasticsearch é›†ç¾¤å¥åº·çŠ¶æ€æ­£å¸¸"
    else
        warn "âŒ Elasticsearch å¥åº·æ£€æŸ¥å¤±è´¥"
    fi
}

# æ˜¾ç¤ºè®¿é—®ä¿¡æ¯
show_access_info() {
    echo
    log "ğŸ‰ ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿéƒ¨ç½²å®Œæˆï¼"
    echo
    info "=== è®¿é—®ä¿¡æ¯ ==="
    info "ğŸ“Š Prometheus (ç›‘æ§æ•°æ®æ”¶é›†):"
    info "   kubectl port-forward -n monitoring svc/prometheus 9090:9090"
    info "   ç„¶åè®¿é—®: http://localhost:9090"
    echo
    info "ğŸ“ˆ Grafana (ç›‘æ§å¯è§†åŒ–):"
    info "   kubectl port-forward -n monitoring svc/grafana 3000:3000"
    info "   ç„¶åè®¿é—®: http://localhost:3000"
    info "   é»˜è®¤è´¦å·: admin / admin123!@#"
    echo
    info "ğŸš¨ AlertManager (å‘Šè­¦ç®¡ç†):"
    info "   kubectl port-forward -n monitoring svc/alertmanager 9093:9093"
    info "   ç„¶åè®¿é—®: http://localhost:9093"
    echo
    info "ğŸ” Kibana (æ—¥å¿—å¯è§†åŒ–):"
    info "   kubectl port-forward -n logging svc/kibana 5601:5601"
    info "   ç„¶åè®¿é—®: http://localhost:5601"
    echo
    info "ğŸ“Š Elasticsearch (æ—¥å¿—å­˜å‚¨):"
    info "   kubectl port-forward -n logging svc/elasticsearch 9200:9200"
    info "   ç„¶åè®¿é—®: http://localhost:9200"
    echo
    info "=== å¸¸ç”¨å‘½ä»¤ ==="
    info "æŸ¥çœ‹ç›‘æ§PodçŠ¶æ€: kubectl get pods -n monitoring"
    info "æŸ¥çœ‹æ—¥å¿—PodçŠ¶æ€: kubectl get pods -n logging"
    info "æŸ¥çœ‹Prometheusé…ç½®: kubectl get configmap prometheus-config -n monitoring -o yaml"
    info "æŸ¥çœ‹åº”ç”¨æ—¥å¿—: kubectl logs -f -l app=figma-frame-faithful -n production"
    echo
    warn "âš ï¸  æ³¨æ„äº‹é¡¹:"
    warn "1. é¦–æ¬¡å¯åŠ¨å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿæ—¶é—´åˆå§‹åŒ–"
    warn "2. è¯·ç¡®ä¿é›†ç¾¤æœ‰è¶³å¤Ÿçš„èµ„æº (è‡³å°‘8GBå†…å­˜ï¼Œ4CPUæ ¸å¿ƒ)"
    warn "3. ç”Ÿäº§ç¯å¢ƒè¯·ä¿®æ”¹é»˜è®¤å¯†ç å’Œé…ç½®æ–‡ä»¶ä¸­çš„é‚®ä»¶/Slackè®¾ç½®"
    warn "4. å»ºè®®é…ç½®æŒä¹…åŒ–å­˜å‚¨ä»¥é˜²æ•°æ®ä¸¢å¤±"
}

# æ¸…ç†å‡½æ•°
cleanup() {
    if [[ "${CLEANUP_ON_ERROR:-false}" == "true" ]]; then
        warn "æ£€æµ‹åˆ°é”™è¯¯ï¼Œæ­£åœ¨æ¸…ç†èµ„æº..."
        kubectl delete namespace monitoring --ignore-not-found=true
        kubectl delete namespace logging --ignore-not-found=true
        kubectl delete storageclass fast-ssd --ignore-not-found=true
    fi
}

# ä¸»å‡½æ•°
main() {
    log "ğŸš€ å¼€å§‹éƒ¨ç½²ç›‘æ§å’Œæ—¥å¿—ç³»ç»Ÿ..."
    
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    while [[ $# -gt 0 ]]; do
        case $1 in
            --enable-ingress)
                export ENABLE_INGRESS=true
                shift
                ;;
            --cleanup-on-error)
                export CLEANUP_ON_ERROR=true
                shift
                ;;
            --monitoring-only)
                export MONITORING_ONLY=true
                shift
                ;;
            --logging-only)
                export LOGGING_ONLY=true
                shift
                ;;
            -h|--help)
                echo "ç”¨æ³•: $0 [é€‰é¡¹]"
                echo "é€‰é¡¹:"
                echo "  --enable-ingress     å¯ç”¨Ingressè®¿é—®"
                echo "  --cleanup-on-error   é”™è¯¯æ—¶æ¸…ç†èµ„æº"
                echo "  --monitoring-only    ä»…éƒ¨ç½²ç›‘æ§ç³»ç»Ÿ"
                echo "  --logging-only       ä»…éƒ¨ç½²æ—¥å¿—ç³»ç»Ÿ"
                echo "  -h, --help          æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯"
                exit 0
                ;;
            *)
                error "æœªçŸ¥å‚æ•°: $1"
                ;;
        esac
    done
    
    # è®¾ç½®é”™è¯¯å¤„ç†
    trap cleanup ERR
    
    # æ‰§è¡Œéƒ¨ç½²æ­¥éª¤
    check_dependencies
    create_namespaces
    deploy_storage_classes
    
    # æ ¹æ®å‚æ•°é€‰æ‹©éƒ¨ç½²å†…å®¹
    if [[ "${LOGGING_ONLY:-false}" != "true" ]]; then
        deploy_prometheus
        deploy_grafana
        deploy_alertmanager
        deploy_node_exporter
    fi
    
    if [[ "${MONITORING_ONLY:-false}" != "true" ]]; then
        deploy_elasticsearch
        deploy_logstash
        deploy_kibana
        deploy_filebeat
    fi
    
    configure_monitoring
    create_ingress
    verify_deployment
    show_access_info
    
    log "ğŸ‰ éƒ¨ç½²å®Œæˆï¼"
}

# è¿è¡Œä¸»å‡½æ•°
main "$@"