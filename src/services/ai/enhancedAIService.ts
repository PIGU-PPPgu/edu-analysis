/**
 * ğŸ¤– å¢å¼ºç‰ˆAIæœåŠ¡
 * é›†æˆæˆæœ¬ç®¡ç†ã€æ™ºèƒ½è·¯ç”±å’Œç¼“å­˜ä¼˜åŒ–çš„ç»Ÿä¸€AIæœåŠ¡æ¥å£
 */

import { toast } from "sonner";
import { EnhancedAIClient } from "../enhancedAIClient";
import { aiCostManager, recordAIUsage } from "./core/aiCostManager";
import { aiRouter, RouteRequest } from "./core/aiRouter";
import { aiCacheManager } from "./core/aiCache";
import { getAPIKey } from "@/utils/apiKeyManager";

// AIè¯·æ±‚æ¥å£
export interface AIRequest {
  messages: Array<{
    role: "system" | "user" | "assistant";
    content: string;
  }>;
  model?: string; // è¯·æ±‚çš„æ¨¡å‹ (å¯é€‰ï¼Œä¼šè¢«è·¯ç”±å™¨ä¼˜åŒ–)
  maxTokens?: number;
  temperature?: number;
  stream?: boolean;
  priority?: "low" | "normal" | "high" | "critical";
  context?: string; // ç”¨äºç¼“å­˜ä¼˜åŒ–
  cacheable?: boolean; // æ˜¯å¦å…è®¸ç¼“å­˜
  requirements?: {
    maxLatency?: number; // æœ€å¤§å»¶è¿Ÿè¦æ±‚
    maxCost?: number; // æœ€å¤§æˆæœ¬è¦æ±‚
    preferredProviders?: string[];
    excludedProviders?: string[];
  };
}

// AIå“åº”æ¥å£
export interface AIResponse {
  content: string;
  finishReason: string;
  usage: {
    promptTokens: number;
    completionTokens: number;
    totalTokens: number;
  };
  metadata: {
    providerId: string;
    modelId: string;
    estimatedCost: number;
    actualLatency: number;
    fromCache: boolean;
    routingReasoning: string;
  };
}

// é”™è¯¯é‡è¯•é…ç½®
interface RetryConfig {
  maxRetries: number;
  baseDelay: number; // åŸºç¡€å»¶è¿Ÿ (ms)
  maxDelay: number; // æœ€å¤§å»¶è¿Ÿ (ms)
  backoffFactor: number; // é€€é¿å› å­
}

/**
 * å¢å¼ºç‰ˆAIæœåŠ¡ç±»
 */
export class EnhancedAIService {
  private defaultRetryConfig: RetryConfig = {
    maxRetries: 3,
    baseDelay: 1000,
    maxDelay: 10000,
    backoffFactor: 2,
  };

  /**
   * ğŸ¯ ç»Ÿä¸€AIè°ƒç”¨æ¥å£
   */
  async chat(request: AIRequest): Promise<AIResponse> {
    const startTime = performance.now();
    const context =
      request.context || this.generateContextKey(request.messages);

    console.log(
      `ğŸ¤– AIè¯·æ±‚å¼€å§‹: ${request.model || "è‡ªåŠ¨é€‰æ‹©"} (ä¼˜å…ˆçº§: ${request.priority || "normal"})`
    );

    try {
      // 1. ç¼“å­˜æ£€æŸ¥
      if (request.cacheable !== false) {
        const cachedResponse = await this.checkCache(context, request);
        if (cachedResponse) {
          console.log(`âš¡ ç¼“å­˜å‘½ä¸­ï¼Œè·³è¿‡AIè°ƒç”¨`);
          return cachedResponse;
        }
      }

      // 2. æ™ºèƒ½è·¯ç”±
      const routeRequest: RouteRequest = {
        modelId: request.model || "gpt-3.5",
        estimatedTokens: this.estimateTokens(request.messages),
        priority: request.priority || "normal",
        context,
        requirements: request.requirements,
      };

      const routeResult = await aiRouter.route(routeRequest);
      console.log(
        `ğŸ¯ è·¯ç”±é€‰æ‹©: ${routeResult.selectedProvider} - ${routeResult.reasoning}`
      );

      // 3. æ‰§è¡ŒAIè°ƒç”¨ï¼ˆå¸¦é‡è¯•ï¼‰
      const response = await this.executeWithRetry(
        request,
        routeResult,
        startTime
      );

      // 4. ç¼“å­˜ç»“æœ
      if (request.cacheable !== false) {
        await this.cacheResponse(context, response, routeResult);
      }

      return response;
    } catch (error) {
      console.error(`âŒ AIè°ƒç”¨å¤±è´¥:`, error);

      // è®°å½•å¤±è´¥
      await this.recordFailure(
        request,
        error as Error,
        performance.now() - startTime
      );

      throw error;
    }
  }

  /**
   * ğŸ”„ å¸¦é‡è¯•çš„AIè°ƒç”¨æ‰§è¡Œ
   */
  private async executeWithRetry(
    request: AIRequest,
    routeResult: any,
    startTime: number,
    retryCount: number = 0
  ): Promise<AIResponse> {
    try {
      const response = await this.executeAICall(
        request,
        routeResult,
        startTime
      );

      // æ›´æ–°æä¾›å•†å¥åº·çŠ¶æ€
      await aiRouter.updateProviderHealth(routeResult.selectedProvider, {
        success: true,
        latency: performance.now() - startTime,
      });

      return response;
    } catch (error) {
      const isRetryable = this.isRetryableError(error as Error);
      const shouldRetry =
        retryCount < this.defaultRetryConfig.maxRetries && isRetryable;

      // æ›´æ–°æä¾›å•†å¥åº·çŠ¶æ€
      await aiRouter.updateProviderHealth(routeResult.selectedProvider, {
        success: false,
        latency: performance.now() - startTime,
        error: (error as Error).message,
      });

      if (shouldRetry) {
        const delay = Math.min(
          this.defaultRetryConfig.baseDelay *
            Math.pow(this.defaultRetryConfig.backoffFactor, retryCount),
          this.defaultRetryConfig.maxDelay
        );

        console.log(
          `ğŸ”„ é‡è¯•AIè°ƒç”¨ (${retryCount + 1}/${this.defaultRetryConfig.maxRetries}) å»¶è¿Ÿ: ${delay}ms`
        );

        await this.sleep(delay);

        // å°è¯•æ•…éšœè½¬ç§»
        if (
          routeResult.fallbackProviders &&
          routeResult.fallbackProviders.length > retryCount
        ) {
          routeResult.selectedProvider =
            routeResult.fallbackProviders[retryCount];
          console.log(`ğŸ”€ æ•…éšœè½¬ç§»åˆ°: ${routeResult.selectedProvider}`);
        }

        return this.executeWithRetry(
          request,
          routeResult,
          startTime,
          retryCount + 1
        );
      }

      throw error;
    }
  }

  /**
   * âš¡ æ‰§è¡Œå®é™…çš„AIè°ƒç”¨
   */
  private async executeAICall(
    request: AIRequest,
    routeResult: any,
    startTime: number
  ): Promise<AIResponse> {
    // è·å–APIå¯†é’¥
    const apiKey = getAPIKey(routeResult.selectedProvider);
    if (!apiKey) {
      throw new Error(`æœªæ‰¾åˆ° ${routeResult.selectedProvider} çš„APIå¯†é’¥`);
    }

    // åˆ›å»ºAIå®¢æˆ·ç«¯
    const client = new EnhancedAIClient(
      apiKey,
      routeResult.selectedProvider,
      routeResult.selectedModel
    );

    // å‡†å¤‡è¯·æ±‚å‚æ•°
    const params = {
      messages: request.messages,
      model: routeResult.selectedModel,
      max_tokens: request.maxTokens || 1000,
      temperature: request.temperature || 0.7,
      stream: request.stream || false,
    };

    // è°ƒç”¨AIæœåŠ¡
    const result = await client.chat.completions.create(params);
    const endTime = performance.now();
    const latency = endTime - startTime;

    // æå–å“åº”æ•°æ®
    const content = result.choices[0]?.message?.content || "";
    const usage = result.usage || {
      prompt_tokens: 0,
      completion_tokens: 0,
      total_tokens: 0,
    };

    // è®°å½•ä½¿ç”¨æƒ…å†µ
    await recordAIUsage({
      providerId: routeResult.selectedProvider,
      modelId: routeResult.selectedModel,
      inputTokens: usage.prompt_tokens,
      outputTokens: usage.completion_tokens,
      requestLatency: latency,
      success: true,
      metadata: {
        priority: request.priority,
        context: request.context,
        estimatedCost: routeResult.estimatedCost,
      },
    });

    // æ„å»ºå“åº”
    const response: AIResponse = {
      content,
      finishReason: result.choices[0]?.finish_reason || "stop",
      usage: {
        promptTokens: usage.prompt_tokens,
        completionTokens: usage.completion_tokens,
        totalTokens: usage.total_tokens,
      },
      metadata: {
        providerId: routeResult.selectedProvider,
        modelId: routeResult.selectedModel,
        estimatedCost: routeResult.estimatedCost,
        actualLatency: latency,
        fromCache: false,
        routingReasoning: routeResult.reasoning,
      },
    };

    console.log(
      `âœ… AIè°ƒç”¨æˆåŠŸ: ${routeResult.selectedProvider} (${latency.toFixed(0)}ms, $${routeResult.estimatedCost.toFixed(4)})`
    );

    return response;
  }

  /**
   * ğŸ” æ£€æŸ¥ç¼“å­˜
   */
  private async checkCache(
    context: string,
    request: AIRequest
  ): Promise<AIResponse | null> {
    const cacheKey = this.generateCacheKey(context, request);
    const cached = await aiCacheManager.get(cacheKey, ["ai-response"]);

    if (cached) {
      // æ„å»ºç¼“å­˜å“åº”
      return {
        ...cached,
        metadata: {
          ...cached.metadata,
          fromCache: true,
        },
      };
    }

    return null;
  }

  /**
   * ğŸ’¾ ç¼“å­˜å“åº”
   */
  private async cacheResponse(
    context: string,
    response: AIResponse,
    routeResult: any
  ): Promise<void> {
    const cacheKey = this.generateCacheKey(context, null);

    await aiCacheManager.set(cacheKey, response, {
      ttl: this.getCacheTTL(response),
      tags: ["ai-response", routeResult.selectedProvider],
      metadata: {
        providerId: routeResult.selectedProvider,
        modelId: routeResult.selectedModel,
        tokenCount: response.usage.totalTokens,
        cost: response.metadata.estimatedCost,
      },
    });
  }

  /**
   * ğŸ“ è®°å½•å¤±è´¥æƒ…å†µ
   */
  private async recordFailure(
    request: AIRequest,
    error: Error,
    latency: number
  ): Promise<void> {
    await recordAIUsage({
      providerId: "unknown",
      modelId: request.model || "unknown",
      inputTokens: this.estimateTokens(request.messages),
      outputTokens: 0,
      requestLatency: latency,
      success: false,
      error: error.message,
      metadata: {
        priority: request.priority,
        context: request.context,
      },
    });
  }

  /**
   * ğŸ”§ å·¥å…·æ–¹æ³•
   */
  private generateContextKey(
    messages: Array<{ role: string; content: string }>
  ): string {
    return messages.map((m) => `${m.role}:${m.content}`).join("|");
  }

  private generateCacheKey(context: string, request: AIRequest | null): string {
    const baseKey = context;
    if (!request) return baseKey;

    const additionalInfo = [
      request.model || "",
      request.maxTokens || "",
      request.temperature || "",
      JSON.stringify(request.requirements || {}),
    ].join("|");

    return `${baseKey}|${additionalInfo}`;
  }

  private estimateTokens(
    messages: Array<{ role: string; content: string }>
  ): number {
    const text = messages.map((m) => m.content).join(" ");
    return Math.ceil(text.length / 4); // ç²—ç•¥ä¼°ç®—ï¼š4å­—ç¬¦ â‰ˆ 1 token
  }

  private getCacheTTL(response: AIResponse): number {
    // æ ¹æ®å“åº”ç±»å‹å’Œå†…å®¹åŠ¨æ€è°ƒæ•´TTL
    if (response.content.length > 1000) {
      return 60 * 60 * 1000; // é•¿å†…å®¹ç¼“å­˜1å°æ—¶
    } else {
      return 30 * 60 * 1000; // çŸ­å†…å®¹ç¼“å­˜30åˆ†é’Ÿ
    }
  }

  private isRetryableError(error: Error): boolean {
    const retryableErrors = [
      "timeout",
      "network",
      "rate limit",
      "server error",
      "503",
      "502",
      "500",
    ];

    const errorMessage = error.message.toLowerCase();
    return retryableErrors.some((keyword) => errorMessage.includes(keyword));
  }

  private sleep(ms: number): Promise<void> {
    return new Promise((resolve) => setTimeout(resolve, ms));
  }

  /**
   * ğŸ“Š è·å–æœåŠ¡ç»Ÿè®¡ä¿¡æ¯
   */
  async getServiceStatistics() {
    const costStats = aiCostManager.getStatistics();
    const cacheStats = aiCacheManager.getStatistics();
    const routerStats = aiRouter.getHealthStats();

    return {
      cost: costStats,
      cache: cacheStats,
      providers: routerStats,
      summary: {
        totalRequests: costStats.totalRequests,
        totalCost: costStats.totalCost,
        avgLatency:
          routerStats.reduce((sum, p) => sum + p.latency, 0) /
          routerStats.length,
        cacheHitRate: cacheStats.hitRate,
        costSavings: cacheStats.costSavings,
      },
    };
  }

  /**
   * ğŸ”§ æ‰¹é‡å¤„ç†æ”¯æŒ
   */
  async batchChat(requests: AIRequest[]): Promise<AIResponse[]> {
    console.log(`ğŸ“¦ æ‰¹é‡å¤„ç† ${requests.length} ä¸ªAIè¯·æ±‚`);

    return aiCacheManager.batchProcess(
      requests,
      async (batchRequests: AIRequest[]) => {
        // å¹¶è¡Œå¤„ç†æ‰¹é‡è¯·æ±‚
        const promises = batchRequests.map((request) => this.chat(request));
        return Promise.all(promises);
      }
    );
  }

  /**
   * ğŸ›ï¸ é…ç½®ç®¡ç†
   */
  setRouterStrategy(strategyType: string): void {
    const strategies = aiRouter.getAvailableStrategies();
    const strategy = strategies.find((s) => s.type === strategyType);

    if (strategy) {
      aiRouter.setStrategy(strategy);
      toast.success(`AIè·¯ç”±ç­–ç•¥å·²åˆ‡æ¢: ${strategy.name}`);
    } else {
      toast.error(`æœªæ‰¾åˆ°è·¯ç”±ç­–ç•¥: ${strategyType}`);
    }
  }

  setCachePolicy(policyName: string): void {
    const policies = aiCacheManager.getAvailablePolicies();
    const policy = policies.find((p) => p.name === policyName);

    if (policy) {
      aiCacheManager.setPolicy(policy);
      toast.success(`AIç¼“å­˜ç­–ç•¥å·²åˆ‡æ¢: ${policy.name}`);
    } else {
      toast.error(`æœªæ‰¾åˆ°ç¼“å­˜ç­–ç•¥: ${policyName}`);
    }
  }

  /**
   * ğŸ§¹ ç»´æŠ¤æ–¹æ³•
   */
  async clearCache(tags?: string[]): Promise<void> {
    await aiCacheManager.clear(tags);
    toast.success("AIç¼“å­˜å·²æ¸…ç©º");
  }

  async exportStatistics(): Promise<any> {
    return {
      timestamp: new Date().toISOString(),
      statistics: await this.getServiceStatistics(),
      configuration: {
        routerStrategy: aiRouter.getCurrentStrategy(),
        cachePolicy: aiCacheManager.getCurrentPolicy(),
        budgets: aiCostManager.getBudgets(),
      },
    };
  }
}

// ğŸŒ å…¨å±€å®ä¾‹
export const enhancedAIService = new EnhancedAIService();

// ğŸ¯ å¿«æ·æ–¹æ³•
export const aiChat = enhancedAIService.chat.bind(enhancedAIService);
export const aiBatchChat = enhancedAIService.batchChat.bind(enhancedAIService);
export const getAIStatistics =
  enhancedAIService.getServiceStatistics.bind(enhancedAIService);
