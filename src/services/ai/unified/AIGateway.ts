/**
 * ğŸšª ç»Ÿä¸€AIç½‘å…³ - æ•´åˆæ‰€æœ‰AIæœåŠ¡çš„ç»Ÿä¸€å…¥å£
 * åŸºäºç°æœ‰aiRouteræ¶æ„ï¼Œæä¾›ç»Ÿä¸€çš„AIæœåŠ¡è®¿é—®æ¥å£
 */

import { aiRouter } from "../core/aiRouter";
import { EnhancedAIClient } from "../../enhancedAIClient";
import { getAllProviders, getProviderConfig } from "../../aiProviderManager";
import {
  AIRequestParams,
  AIAnalysisResult,
  ProviderConfig,
  AIRequestOptions,
} from "../../../types/ai";
import { getUserAIConfig, getUserAPIKey } from "../../../utils/userAuth";
import { logInfo, logError } from "../../../utils/logger";
import { toast } from "sonner";
import { aiCache } from "./AICache";
import { aiPerformanceMonitor } from "./AIPerformanceMonitor";

/**
 * ç»Ÿä¸€AIè¯·æ±‚æ¥å£
 */
export interface UnifiedAIRequest {
  content: string;
  requestType:
    | "analysis"
    | "chat"
    | "image_analysis"
    | "grade_analysis"
    | "homework_analysis";
  context?: {
    subject?: string;
    homeworkId?: string;
    existingKnowledgePoints?: any[];
    conversationHistory?: { role: string; content: string }[];
  };
  options?: {
    temperature?: number;
    maxTokens?: number;
    priority?: "low" | "normal" | "high" | "critical";
    preferredProviders?: string[];
    excludedProviders?: string[];
  };
}

/**
 * ç»Ÿä¸€AIå“åº”æ¥å£
 */
export interface UnifiedAIResponse {
  success: boolean;
  content: string;
  metadata?: {
    provider: string;
    model: string;
    tokensUsed?: number;
    responseTime: number;
    confidence: number;
    cost?: number;
    cached?: boolean;
    cacheHit?: boolean;
  };
  knowledgePoints?: any[];
  error?: string;
}

/**
 * ç»Ÿä¸€AIç½‘å…³ç±»
 */
export class AIGateway {
  private static instance: AIGateway;
  private initialized = false;

  private constructor() {}

  /**
   * è·å–å•ä¾‹å®ä¾‹
   */
  public static getInstance(): AIGateway {
    if (!AIGateway.instance) {
      AIGateway.instance = new AIGateway();
    }
    return AIGateway.instance;
  }

  /**
   * åˆå§‹åŒ–ç½‘å…³
   */
  async initialize(): Promise<void> {
    if (this.initialized) return;

    try {
      logInfo("ğŸšª æ­£åœ¨åˆå§‹åŒ–AIç½‘å…³...");

      // éªŒè¯aiRouterå¯ç”¨æ€§
      const healthStats = aiRouter.getHealthStats();
      logInfo(`ğŸ“Š AIè·¯ç”±å™¨çŠ¶æ€: ${healthStats.length}ä¸ªæä¾›å•†`);

      // éªŒè¯æä¾›å•†é…ç½®
      const providers = getAllProviders();
      const providerCount = Object.keys(providers).length;
      logInfo(`âš™ï¸ å¯ç”¨æä¾›å•†: ${providerCount}ä¸ª`);

      this.initialized = true;
      logInfo("âœ… AIç½‘å…³åˆå§‹åŒ–å®Œæˆ");
    } catch (error) {
      logError("âŒ AIç½‘å…³åˆå§‹åŒ–å¤±è´¥:", error);
      throw new Error(`AIç½‘å…³åˆå§‹åŒ–å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ¯ ç»Ÿä¸€AIè¯·æ±‚å¤„ç†å…¥å£
   */
  async processRequest(request: UnifiedAIRequest): Promise<UnifiedAIResponse> {
    if (!this.initialized) {
      await this.initialize();
    }

    const startTime = Date.now();

    try {
      logInfo("ğŸ¯ å¼€å§‹å¤„ç†AIè¯·æ±‚", {
        type: request.requestType,
        contentLength: request.content.length,
        priority: request.options?.priority || "normal",
      });

      // ğŸ—„ï¸ é¦–å…ˆæ£€æŸ¥ç¼“å­˜
      const cachedResponse = aiCache.get(request);
      if (cachedResponse) {
        logInfo("ğŸ¯ ç¼“å­˜å‘½ä¸­ï¼Œç›´æ¥è¿”å›ç»“æœ", {
          type: request.requestType,
          responseTime: Date.now() - startTime,
        });
        return cachedResponse;
      }

      // ä½¿ç”¨AIè·¯ç”±å™¨é€‰æ‹©æœ€ä½³æä¾›å•†
      const routeRequest = {
        modelId: this.getModelForRequestType(request.requestType),
        estimatedTokens: this.estimateTokens(request.content),
        priority: request.options?.priority || "normal",
        context: JSON.stringify(request.context || {}),
        requirements: {
          maxLatency: this.getMaxLatencyForType(request.requestType),
          preferredProviders: request.options?.preferredProviders,
          excludedProviders: request.options?.excludedProviders,
        },
      };

      const routeResult = await aiRouter.route(routeRequest);

      logInfo("ğŸ“ è·¯ç”±é€‰æ‹©ç»“æœ", {
        provider: routeResult.selectedProvider,
        model: routeResult.selectedModel,
        confidence: routeResult.confidence,
      });

      // åˆ›å»ºAIå®¢æˆ·ç«¯
      const client = await this.createClient(
        routeResult.selectedProvider,
        routeResult.selectedModel
      );

      // æ ¹æ®è¯·æ±‚ç±»å‹å¤„ç†
      let result: UnifiedAIResponse;

      switch (request.requestType) {
        case "analysis":
          result = await this.handleAnalysisRequest(
            client,
            request,
            routeResult
          );
          break;
        case "chat":
          result = await this.handleChatRequest(client, request, routeResult);
          break;
        case "image_analysis":
          result = await this.handleImageAnalysisRequest(
            client,
            request,
            routeResult
          );
          break;
        case "grade_analysis":
          result = await this.handleGradeAnalysisRequest(
            client,
            request,
            routeResult
          );
          break;
        case "homework_analysis":
          result = await this.handleHomeworkAnalysisRequest(
            client,
            request,
            routeResult
          );
          break;
        default:
          throw new Error(`ä¸æ”¯æŒçš„è¯·æ±‚ç±»å‹: ${request.requestType}`);
      }

      // æ›´æ–°æä¾›å•†å¥åº·çŠ¶æ€
      const responseTime = Date.now() - startTime;
      await aiRouter.updateProviderHealth(routeResult.selectedProvider, {
        success: result.success,
        latency: responseTime,
        error: result.error,
      });

      // è®¾ç½®å“åº”å…ƒæ•°æ®
      result.metadata = {
        provider: routeResult.selectedProvider,
        model: routeResult.selectedModel,
        responseTime,
        confidence: routeResult.confidence * 100,
        cost: routeResult.estimatedCost,
        cached: false,
        cacheHit: false,
        ...result.metadata,
      };

      // ğŸ—„ï¸ å¦‚æœè¯·æ±‚æˆåŠŸï¼Œå­˜å‚¨åˆ°ç¼“å­˜
      if (result.success) {
        aiCache.set(request, result);
        logInfo("ğŸ—„ï¸ å“åº”å·²å­˜å‚¨åˆ°ç¼“å­˜", {
          type: request.requestType,
          contentLength: result.content.length,
        });
      }

      logInfo("âœ… AIè¯·æ±‚å¤„ç†å®Œæˆ", {
        success: result.success,
        responseTime,
        provider: routeResult.selectedProvider,
      });

      // ğŸ“Š è®°å½•æ€§èƒ½æŒ‡æ ‡
      aiPerformanceMonitor.recordRequest(request, result, startTime);

      return result;
    } catch (error) {
      const responseTime = Date.now() - startTime;
      logError("âŒ AIè¯·æ±‚å¤„ç†å¤±è´¥:", error);

      const errorResult = {
        success: false,
        content: "",
        error: error.message || "æœªçŸ¥é”™è¯¯",
        metadata: {
          provider: "unknown",
          model: "unknown",
          responseTime,
          confidence: 0,
          cached: false,
          cacheHit: false,
        },
      };

      // ğŸ“Š è®°å½•é”™è¯¯çš„æ€§èƒ½æŒ‡æ ‡
      aiPerformanceMonitor.recordRequest(request, errorResult, startTime);

      return errorResult;
    }
  }

  /**
   * ğŸ“ å¤„ç†æ–‡æœ¬åˆ†æè¯·æ±‚
   */
  private async handleAnalysisRequest(
    client: EnhancedAIClient,
    request: UnifiedAIRequest,
    routeResult: any
  ): Promise<UnifiedAIResponse> {
    try {
      const systemPrompt = `ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„æ•™è‚²å†…å®¹åˆ†æä¸“å®¶ï¼Œæ“…é•¿è¯†åˆ«å­¦ä¹ å†…å®¹ä¸­çš„å…³é”®ä¿¡æ¯å’ŒçŸ¥è¯†ç‚¹ã€‚
è¯·åˆ†æä»¥ä¸‹å†…å®¹å¹¶æä¾›æœ‰ä»·å€¼çš„æ´å¯Ÿã€‚`;

      const response = await client.chat.completions.create({
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: request.content },
        ],
        temperature: request.options?.temperature || 0.3,
        max_tokens: request.options?.maxTokens || 2000,
      });

      const content = response.choices[0]?.message?.content || "";

      return {
        success: true,
        content,
        metadata: {
          tokensUsed: this.estimateTokens(content),
        },
      };
    } catch (error) {
      throw new Error(`æ–‡æœ¬åˆ†æå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ’¬ å¤„ç†èŠå¤©å¯¹è¯è¯·æ±‚
   */
  private async handleChatRequest(
    client: EnhancedAIClient,
    request: UnifiedAIRequest,
    routeResult: any
  ): Promise<UnifiedAIResponse> {
    try {
      const messages = [];

      // æ·»åŠ ç³»ç»Ÿæç¤º
      messages.push({
        role: "system",
        content:
          "ä½ æ˜¯ä¸€ä¸ªæ•™è‚²AIåŠ©æ‰‹ï¼Œä¸“é—¨å¸®åŠ©æ•™å¸ˆåˆ†æå­¦ç”Ÿæ•°æ®å’Œæä¾›æ•™å­¦å»ºè®®ã€‚è¯·ç”¨ç®€æ´ä¸“ä¸šçš„è¯­è¨€å›ç­”ã€‚",
      });

      // æ·»åŠ å¯¹è¯å†å²
      if (request.context?.conversationHistory) {
        messages.push(...request.context.conversationHistory);
      }

      // æ·»åŠ å½“å‰æ¶ˆæ¯
      messages.push({ role: "user", content: request.content });

      const response = await client.chat.completions.create({
        messages,
        temperature: request.options?.temperature || 0.7,
        max_tokens: request.options?.maxTokens || 1000,
      });

      const content = response.choices[0]?.message?.content || "";

      return {
        success: true,
        content,
        metadata: {
          tokensUsed: this.estimateTokens(content),
        },
      };
    } catch (error) {
      throw new Error(`èŠå¤©å¯¹è¯å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ–¼ï¸ å¤„ç†å›¾åƒåˆ†æè¯·æ±‚
   */
  private async handleImageAnalysisRequest(
    client: EnhancedAIClient,
    request: UnifiedAIRequest,
    routeResult: any
  ): Promise<UnifiedAIResponse> {
    try {
      const systemPrompt = `ä½ æ˜¯ä¸€ä½æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿åˆ†ææ•™è‚²å›¾ç‰‡å†…å®¹å¹¶æå–çŸ¥è¯†ç‚¹ã€‚
è¯·åˆ†æå›¾ç‰‡ä¸­çš„ä½œä¸šå†…å®¹ï¼Œè¯†åˆ«å‡ºåŒ…å«çš„çŸ¥è¯†ç‚¹ã€‚

ç§‘ç›®: ${request.context?.subject || "æœªæŒ‡å®š"}

ä»¥JSONæ ¼å¼è¿”å›åˆ†æç»“æœ:
{
  "knowledgePoints": [
    {
      "name": "çŸ¥è¯†ç‚¹åç§°",
      "description": "çŸ¥è¯†ç‚¹æè¿°",
      "importance": 1-5,
      "confidence": 0-100
    }
  ]
}`;

      // æ„å»ºå¤šæ¨¡æ€æ¶ˆæ¯
      const messages = [
        { role: "system", content: systemPrompt },
        {
          role: "user",
          content: [
            { type: "text", text: "è¯·åˆ†æè¿™å¼ å›¾ç‰‡ä¸­çš„æ•™è‚²å†…å®¹" },
            { type: "image_url", image_url: { url: request.content } },
          ],
        },
      ];

      const response = await client.chat.completions.create({
        messages,
        temperature: 0.3,
        max_tokens: 2000,
      });

      const content = response.choices[0]?.message?.content || "";

      // å°è¯•è§£æJSONæ ¼å¼çš„çŸ¥è¯†ç‚¹
      let knowledgePoints = [];
      try {
        const jsonMatch = content.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          const parsed = JSON.parse(jsonMatch[0]);
          knowledgePoints = parsed.knowledgePoints || [];
        }
      } catch (parseError) {
        logError("è§£æçŸ¥è¯†ç‚¹JSONå¤±è´¥:", parseError);
      }

      return {
        success: true,
        content,
        knowledgePoints,
        metadata: {
          tokensUsed: this.estimateTokens(content),
        },
      };
    } catch (error) {
      throw new Error(`å›¾åƒåˆ†æå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ“Š å¤„ç†æˆç»©åˆ†æè¯·æ±‚
   */
  private async handleGradeAnalysisRequest(
    client: EnhancedAIClient,
    request: UnifiedAIRequest,
    routeResult: any
  ): Promise<UnifiedAIResponse> {
    try {
      const systemPrompt = `ä½ æ˜¯ä¸€ä½æ•™è‚²æ•°æ®åˆ†æä¸“å®¶ï¼Œæ“…é•¿åˆ†æå­¦ç”Ÿæˆç»©æ•°æ®å¹¶æä¾›æ•™å­¦æ´å¯Ÿã€‚
è¯·åˆ†æä»¥ä¸‹æˆç»©æ•°æ®ï¼Œæä¾›æœ‰ä»·å€¼çš„åˆ†æç»“æœå’Œå»ºè®®ã€‚

è¯·å…³æ³¨ä»¥ä¸‹æ–¹é¢ï¼š
1. æˆç»©åˆ†å¸ƒæƒ…å†µ
2. å­¦ç§‘å¼ºå¼±é¡¹åˆ†æ
3. å­¦ä¹ è¶‹åŠ¿è¯†åˆ«
4. æ”¹è¿›å»ºè®®`;

      const response = await client.chat.completions.create({
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: request.content },
        ],
        temperature: 0.2,
        max_tokens: 2500,
      });

      const content = response.choices[0]?.message?.content || "";

      return {
        success: true,
        content,
        metadata: {
          tokensUsed: this.estimateTokens(content),
        },
      };
    } catch (error) {
      throw new Error(`æˆç»©åˆ†æå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ“ å¤„ç†ä½œä¸šåˆ†æè¯·æ±‚
   */
  private async handleHomeworkAnalysisRequest(
    client: EnhancedAIClient,
    request: UnifiedAIRequest,
    routeResult: any
  ): Promise<UnifiedAIResponse> {
    try {
      const existingPoints = request.context?.existingKnowledgePoints || [];
      const existingPointsText =
        existingPoints.length > 0
          ? existingPoints.map((p) => `- ${p.name}`).join("\n")
          : "(æ— )";

      const systemPrompt = `ä½ æ˜¯ä¸€ä½æ•™è‚²ä¸“å®¶ï¼Œæ“…é•¿åˆ†æä½œä¸šå†…å®¹å¹¶è¯†åˆ«çŸ¥è¯†ç‚¹ã€‚

ç§‘ç›®: ${request.context?.subject || "æœªæŒ‡å®š"}
å·²æœ‰çŸ¥è¯†ç‚¹:
${existingPointsText}

è¯·åˆ†æä½œä¸šå†…å®¹ï¼Œè¯†åˆ«æ–°çš„çŸ¥è¯†ç‚¹ã€‚ä»¥JSONæ ¼å¼è¿”å›:
{
  "knowledgePoints": [
    {
      "name": "çŸ¥è¯†ç‚¹åç§°",
      "description": "çŸ¥è¯†ç‚¹æè¿°",
      "importance": 1-5,
      "masteryLevel": 1-5,
      "confidence": 0-100,
      "isNew": true
    }
  ]
}`;

      const response = await client.chat.completions.create({
        messages: [
          { role: "system", content: systemPrompt },
          { role: "user", content: request.content },
        ],
        temperature: 0.3,
        max_tokens: 2000,
      });

      const content = response.choices[0]?.message?.content || "";

      // è§£æçŸ¥è¯†ç‚¹
      let knowledgePoints = [];
      try {
        const jsonMatch = content.match(/\{[\s\S]*\}/);
        if (jsonMatch) {
          const parsed = JSON.parse(jsonMatch[0]);
          knowledgePoints = parsed.knowledgePoints || [];
        }
      } catch (parseError) {
        logError("è§£æä½œä¸šçŸ¥è¯†ç‚¹JSONå¤±è´¥:", parseError);
      }

      return {
        success: true,
        content,
        knowledgePoints,
        metadata: {
          tokensUsed: this.estimateTokens(content),
        },
      };
    } catch (error) {
      throw new Error(`ä½œä¸šåˆ†æå¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ”§ åˆ›å»ºAIå®¢æˆ·ç«¯
   */
  private async createClient(
    providerId: string,
    modelId: string
  ): Promise<EnhancedAIClient> {
    try {
      // è·å–APIå¯†é’¥
      const apiKey = await getUserAPIKey(providerId);
      if (!apiKey) {
        throw new Error(`æœªæ‰¾åˆ°${providerId}çš„APIå¯†é’¥`);
      }

      // åˆ›å»ºå¢å¼ºå®¢æˆ·ç«¯
      const client = new EnhancedAIClient(
        apiKey,
        providerId,
        modelId,
        true // å¯ç”¨è°ƒè¯•æ¨¡å¼
      );

      return client;
    } catch (error) {
      throw new Error(`åˆ›å»ºAIå®¢æˆ·ç«¯å¤±è´¥: ${error.message}`);
    }
  }

  /**
   * ğŸ“ ä¼°ç®—tokenæ•°é‡
   */
  private estimateTokens(text: string): number {
    // ç®€å•ä¼°ç®—ï¼šä¸­æ–‡æŒ‰å­—ç¬¦æ•°ï¼Œè‹±æ–‡æŒ‰å•è¯æ•°*1.3
    const chineseChars = (text.match(/[\u4e00-\u9fff]/g) || []).length;
    const englishWords = text
      .replace(/[\u4e00-\u9fff]/g, "")
      .split(/\s+/)
      .filter(Boolean).length;

    return Math.ceil(chineseChars + englishWords * 1.3);
  }

  /**
   * ğŸ¯ æ ¹æ®è¯·æ±‚ç±»å‹è·å–æ¨èæ¨¡å‹
   */
  private getModelForRequestType(requestType: string): string {
    const modelMap = {
      analysis: "gpt-4",
      chat: "gpt-3.5",
      image_analysis: "gpt-4-vision",
      grade_analysis: "gpt-4",
      homework_analysis: "gpt-4",
    };

    return modelMap[requestType] || "gpt-3.5";
  }

  /**
   * â±ï¸ æ ¹æ®è¯·æ±‚ç±»å‹è·å–æœ€å¤§å»¶è¿Ÿ
   */
  private getMaxLatencyForType(requestType: string): number {
    const latencyMap = {
      analysis: 10000, // 10ç§’
      chat: 5000, // 5ç§’
      image_analysis: 15000, // 15ç§’
      grade_analysis: 12000, // 12ç§’
      homework_analysis: 10000, // 10ç§’
    };

    return latencyMap[requestType] || 8000;
  }

  /**
   * ğŸ“Š è·å–ç½‘å…³çŠ¶æ€
   */
  getStatus() {
    return {
      initialized: this.initialized,
      routerStats: aiRouter.getHealthStats(),
      currentStrategy: aiRouter.getCurrentStrategy(),
      availableProviders: Object.keys(getAllProviders()).length,
      cacheStats: aiCache.getStats(),
      cacheInfo: aiCache.getInfo(),
      performanceMetrics: aiPerformanceMonitor.getMetrics(),
    };
  }

  /**
   * ğŸ—„ï¸ è·å–ç¼“å­˜æ€§èƒ½æŠ¥å‘Š
   */
  getCacheReport() {
    return aiCache.getPerformanceReport();
  }

  /**
   * ğŸ§¹ æ¸…ç†ç¼“å­˜
   */
  clearCache(requestType?: string) {
    if (requestType) {
      aiCache.clearByType(requestType);
      logInfo("ğŸ§¹ å·²æ¸…ç†ç‰¹å®šç±»å‹ç¼“å­˜", { requestType });
    } else {
      aiCache.clear();
      logInfo("ğŸ§¹ å·²æ¸…ç†å…¨éƒ¨ç¼“å­˜");
    }
  }

  /**
   * âš¡ é¢„çƒ­ç¼“å­˜
   */
  async warmupCache(commonRequests: UnifiedAIRequest[]): Promise<void> {
    await aiCache.warmup(commonRequests);
    logInfo("âš¡ ç¼“å­˜é¢„çƒ­å®Œæˆ", { requestCount: commonRequests.length });
  }

  /**
   * ğŸ“ˆ è·å–æ€§èƒ½æŒ‡æ ‡
   */
  getPerformanceMetrics(timeRangeMs?: number) {
    return aiPerformanceMonitor.getMetrics(timeRangeMs);
  }

  /**
   * ğŸ“Š è·å–æ€§èƒ½è¶‹åŠ¿æ•°æ®
   */
  getPerformanceTrends(intervalMs?: number) {
    return aiPerformanceMonitor.getTrendData(intervalMs);
  }

  /**
   * âš ï¸ è·å–æœ€è¿‘é”™è¯¯
   */
  getRecentErrors(limit?: number) {
    return aiPerformanceMonitor.getRecentErrors(limit);
  }

  /**
   * ğŸ’¡ è·å–æ€§èƒ½å»ºè®®
   */
  getPerformanceRecommendations() {
    return aiPerformanceMonitor.getPerformanceRecommendations();
  }

  /**
   * ğŸ§¹ æ¸…ç†æ€§èƒ½æ•°æ®
   */
  cleanupPerformanceData(olderThanMs?: number) {
    aiPerformanceMonitor.cleanup(olderThanMs);
    logInfo("ğŸ§¹ æ€§èƒ½æ•°æ®æ¸…ç†å®Œæˆ");
  }

  /**
   * ğŸ”„ é‡æ–°åˆå§‹åŒ–ç½‘å…³
   */
  async reinitialize(): Promise<void> {
    this.initialized = false;
    await this.initialize();
  }
}

// å¯¼å‡ºå•ä¾‹å®ä¾‹
export const aiGateway = AIGateway.getInstance();
